# Calculate aggregate statistics (e.g., average or sum) using Spark
#sum
val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
ordersRDD.count()

val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")
val orderItemsMap = orderItemsRDD.map(rec => (rec.split(",")(4).toDouble))
orderItemsMap.take(5).foreach(println)

val orderItemsReduce = orderItemsMap.reduce((acc, value) => acc + value)

#Get max priced product from products table
#There is one record which is messing up default , delimiters
#Clean it up (we will see how we can filter with out deleting the record later)
hadoop fs -get /user/cloudera/sqoop_import/products
#Delete the record with product_id 685
hadoop fs -put -f products/part* /user/cloudera/sqoop_import/products

#pyspark script to get the max priced product
val productsRDD = sc.textFile("/user/cloudera/sqoop_import/products")
val productsMap = productsRDD.map(rec => rec)
productsMap.reduce((rec1, rec2) => (
  if(rec1.split(",")(4).toFloat >= rec2.split(",")(4).toFloat)
    rec1
  else 
    rec2)
)

#avg
val revenue = sc.textFile("/user/cloudera/sqoop_import/order_items").
  map(rec => rec.split(",")(4).toDouble).
  reduce((rev1, rev2) => rev1 + rev2)
val totalOrders = sc.textFile("/user/cloudera/sqoop_import/order_items").
  map(rec => rec.split(",")(1).toInt).
  distinct().
  count()

#Number of orders by status
val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
val ordersMap = ordersRDD.map(rec => (rec.split(",")(3), 1))
ordersMap.countByKey().foreach(println)

#groupByKey is not very efficient for aggregations. It does not use combiner
val ordersByStatus = ordersMap.groupByKey().map(t => (t._1, t._2.sum))

#reduceByKey uses combiner - both reducer logic and combiner logic are same
val ordersByStatus = ordersMap.reduceByKey((acc, value) => acc + value)

#combineByKey can be used when reduce logic and combine logic are different
#Both reduceByKey and combineByKey expects type of input data and output data are same
val ordersByStatus = ordersMap.combineByKey(value => 1, (acc: Int, value: Int) => acc+value,  (acc: Int, value: Int) => acc+value)

#aggregateByKey can be used when reduce logic and combine logic is different
#Also type of input data and output data need not be same
val ordersMap = ordersRDD.map(rec =>  (rec.split(",")(3), rec))
val ordersByStatus = ordersMap.aggregateByKey(0, (acc, value) => acc+1, (acc, value) => acc+value)
ordersByStatus.collect().foreach(println)

#Number of orders by order date and order status
#Key orderDate and orderStatus
val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
val ordersMapRDD = ordersRDD.map(rec => ((rec.split(",")(1), rec.split(",")(3)), 1))
val ordersByStatusPerDay = ordersMapRDD.reduceByKey((v1, v2) => v1+v2)

ordersByStatusPerDay.collect().foreach(println)

#Total Revenue per day
val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")

val ordersParsedRDD = ordersRDD.map(rec => (rec.split(",")(0), rec))
val orderItemsParsedRDD = orderItemsRDD.map(rec => (rec.split(",")(1), rec))

val ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
val ordersJoinOrderItemsMap = ordersJoinOrderItems.map(t => (t._2._2.split(",")(1), t._2._1.split(",")(4).toFloat))

val revenuePerDay = ordersJoinOrderItemsMap.reduceByKey((acc, value) => acc + value)
revenuePerDay.collect().foreach(println)

#average
#average revenue per day
#Parse Orders (key order_id)
#Parse Order items (key order_item_order_id)
#Join the data sets
#Parse joined data and get (order_date, order_id) as key  and order_item_subtotal as value
#Use appropriate aggregate function to get sum(order_item_subtotal) for each order_date, order_id combination
#Parse data to discard order_id and get order_date as key and sum(order_item_subtotal) per order as value
#Use appropriate aggregate function to get sum(order_item_subtotal) per day and count(distinct order_id) per day
#Parse data and apply average logic
val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")

val ordersParsedRDD = ordersRDD.map(rec => (rec.split(",")(0), rec))
val orderItemsParsedRDD = orderItemsRDD.map(rec => (rec.split(",")(1), rec))

val ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
val ordersJoinOrderItemsMap = ordersJoinOrderItems.map(t => ((t._2._2.split(",")(1), t._1), t._2._1.split(",")(4).toFloat))

val revenuePerDayPerOrder = ordersJoinOrderItemsMap.reduceByKey((acc, value) => acc + value)
val revenuePerDayPerOrderMap = revenuePerDayPerOrder.map(rec => (rec._1._1, rec._2))

val revenuePerDay = revenuePerDayPerOrderMap.aggregateByKey((0.0, 0))(
(acc, revenue) => (acc._1 + revenue, acc._2 + 1), 
(total1, total2) => (total1._1 + total2._1, total1._2 + total2._2) 
)

revenuePerDay.collect().foreach(println)

val avgRevenuePerDay = revenuePerDay.map(x => (x._1, x._2._1/x._2._2))

#Customer id with max revenue
val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")

val ordersParsedRDD = ordersRDD.map(rec => (rec.split(",")(0), rec))
val orderItemsParsedRDD = orderItemsRDD.map(rec => (rec.split(",")(1), rec))

val ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
val ordersPerDayPerCustomer = ordersJoinOrderItems.map(rec => ((rec._2._2.split(",")(1), rec._2._2.split(",")(2)), rec._2._1.split(",")(4).toFloat))
val revenuePerDayPerCustomer = ordersPerDayPerCustomer.reduceByKey((x, y) => x + y)

val revenuePerDayPerCustomerMap = revenuePerDayPerCustomer.map(rec => (rec._1._1, (rec._1._2, rec._2)))
val topCustomerPerDaybyRevenue = revenuePerDayPerCustomerMap.reduceByKey((x, y) => (if(x._2 >= y._2) x else y))

#Using regular function
def findMax(x: (String, Float), y: (String, Float)): (String, Float) = {
  if(x._2 >= y._2)
    return x
  else
    return y
}

val topCustomerPerDaybyRevenue = revenuePerDayPerCustomerMap.reduceByKey((x, y) => findMax(x, y))

# Using Hive Context
import org.apache.spark.sql.hive.HiveContext
val hiveContext = new HiveContext(sc)
hiveContext.sql("set spark.sql.shuffle.partitions=10");

hiveContext.sql("select o.order_date, sum(oi.order_item_subtotal)/count(distinct oi.order_item_order_id) from orders o join order_items oi on o.order_id = oi.order_item_order_id group by o.order_date order by o.order_date").collect().foreach(println)

# This query works in hive
select * from (select q.order_date, q.order_customer_id, q.order_item_subtotal, 
max(q.order_item_subtotal) over (partition by q.order_date) max_order_item_subtotal 
from (select o.order_date, o.order_customer_id, sum(oi.order_item_subtotal) order_item_subtotal 
from orders o join order_items oi 
on o.order_id = oi.order_item_order_id 
group by o.order_date, o.order_customer_id) q) s
where s.order_item_subtotal = s.max_order_item_subtotal
order by s.order_date;

select * from (
select o.order_date, o.order_customer_id, sum(oi.order_item_subtotal) order_item_subtotal 
from orders o join order_items oi 
on o.order_id = oi.order_item_order_id 
group by o.order_date, o.order_customer_id) q1
join
(select q.order_date, max(q.order_item_subtotal) order_item_subtotal
from (select o.order_date, o.order_customer_id, sum(oi.order_item_subtotal) order_item_subtotal
from orders o join order_items oi
on o.order_id = oi.order_item_order_id
group by o.order_date, o.order_customer_id) q
group by q.order_date) q2
on q1.order_date = q2.order_date and q1.order_item_subtotal = q2.order_item_subtotal
order by q1.order_date;