# Load data from HDFS and storing results back to HDFS using Spark
import org.apache.spark.SparkContext

val dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
dataRDD.collect().foreach(println)

dataRDD.count()

dataRDD.saveAsTextFile("/user/cloudera/scalaspark/departments")

#Object files are not available in python
dataRDD.saveAsObjectFile("/user/cloudera/scalaspark/departmentsObject")

#saveAsSequenceFile
import org.apache.hadoop.io._
dataRDD.map(x => (NullWritable.get(), x)).saveAsSequenceFile("/user/cloudera/scalaspark/departmentsSeq")
dataRDD.map(x => (x.split(",")(0), x.split(",")(1))).saveAsSequenceFile("/user/cloudera/scalaspark/departmentsSeq")

import org.apache.hadoop.mapreduce.lib.output._

val path="/user/cloudera/scalaspark/departmentsSeq"
dataRDD.map(x => (new Text(x.split(",")(0)), new Text(x.split(",")(1)))).saveAsNewAPIHadoopFile(path, classOf[Text], classOf[Text], classOf[SequenceFileOutputFormat[Text, Text]])

#reading sequence file
sc.sequenceFile("/user/cloudera/spark/departmentsSeq", classOf[IntWritable], classOf[Text]).map(rec => rec.toString()).collect().foreach(println)

import org.apache.spark.sql.hive.HiveContext
val sqlContext = new HiveContext(sc)
val depts = sqlContext.sql("select * from departments")
depts.collect().foreach(println)

sqlContext.sql("create table departmentsScalaSpark as select * from departments")
val depts = sqlContext.sql("select * from departmentsScalaSpark")
depts.collect().foreach(println)

#We can run hive INSERT, LOAD and any valid hive query in Hive context

#Make sure you copy departments.json to HDFS
#create departments.json on Linux file system
<record>
<department_id>2</department_id>
<department_name>Fitness</department_name>
</record>
{"department_id":2, "department_name":"Fitness"}
{"department_id":3, "department_name":"Footwear"}
{"department_id":4, "department_name":"Apparel"}
{"department_id":5, "department_name":"Golf"}
{"department_id":6, "department_name":"Outdoors"}
{"department_id":7, "department_name":"Fan Shop"}
{"department_id":8, "department_name":"TESTING"}
{"department_id":8000, "department_name":"TESTING"}

#copying to HDFS (using linux command line)
hadoop fs -put departments.json /user/cloudera/scalaspark

import org.apache.spark.sql.SQLContext
val sqlContext = new SQLContext(sc)
val departmentsJson = sqlContext.jsonFile("/user/cloudera/scalaspark/departments.json")
departmentsJson.registerTempTable("departmentsTable")
val departmentsData = sqlContext.sql("select * from departmentsTable")
departmentsData.collect().foreach(println)

#Writing data in json format
departmentsData.toJSON.saveAsTextFile("/user/cloudera/scalaspark/departmentsJson")

#Validating the data
hadoop fs -cat /user/cloudera/scalaspark/departmentsJson/part*